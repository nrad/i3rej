
docs: |
  Quickstart configuration for TFXKit experiments.
  This is example configuation based on the breast cancer dataset of sklearn.
  It includes a simple model architecture, basic data settings, and a short training duration.
  You can modify this configuration to suit your specific needs.

defaults:
  - default_config  

info:
  model_name: "MuEmb"
  save_dir: "save_dir_model"

data:
  labels: ["CscdBDT"]
  # test_files: "/Users/nrad/Work/data/hdf/Cscd_v0.0.12/20904/test_train_nocombineskimmed_nphotons/test.hdf5"
  test_files: "/Users/nrad/Work/data/hdf/Cscd_v0.0.12/20904/test_train_nocombineskimmed_nphotons/test_small.hdf5"


model:
  function: custom_model.define_muemb_model
  overwrite: true
  reload_model: true

  parameters:
    event_branch_layers: [64,64]
    muon_branch_layers: [512]
    combination_layers: [64,64]
    muon_embedding_dim: 16          
    hidden_activation: relu
    dropout: 0.7
    dropout_muon: null
    dropout_event: null
    kernel_regularizer: 1e-4
    aggregation_method: simple3
    batch_size: null

training:
  epochs: 10
  batch_size: 32000
  validation_split: 0.2

optimizer:
  parameters:
    learning_rate: 0.001


plotter:
  # plots_path: tfxkit_results/plots
  weight_column: sel_flux_weights
  weight_column_train: 
    - sel_flux_weights
    - sample_weight
    - 0.0225

  functions:
    plot_speedup:
      function: speedup_utils.plot_speedup

  sequence:
    - plot_speedup
    - plot_history
    - plot_predictions
    - plot_roc


tuning_data:
tuner:
  functions:
    generic_tuner:
      parameters:
        model.parameters.event_branch_layers: [ "64", "512", "[64, 64]", "[128, 128]", "[512, 512]" ]
        model.parameters.muon_branch_layers: [ "64", "512", "[64, 64]", "[128, 128]", "[512, 512]" ]
        model.parameters.combination_layers: [ "64", "512", "[64, 64]", "[128, 128]", "[512, 512]" ]
        model.parameters.muon_embedding_dim: [ "16", "32", "128" ]
        # optimizer.parameters.epsilon: [1e-7, 1e-8, 1e-9]
        # optimizer.parameters.amsgrad: [true, false]
        # optimizer.parameters.beta_1: [0.9, 0.99]
        # optimizer.parameters.beta_2: [0.999, 0.9999]

  tuner:
    function: keras_tuner.BayesianOptimization
    parameters:
        alpha: 0.001
        beta: 2.6
        # directory: tfxkit_results/HPTunning/
        max_trials: 20
  search:
    epochs: 5
    
